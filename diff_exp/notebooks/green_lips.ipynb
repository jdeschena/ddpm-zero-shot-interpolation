{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b7c39d0-cca8-4a0f-aea8-afe8a6cb505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from diff_exp.data.attribute_celeba_dataset import default_args, Dataset\n",
    "from omegaconf import OmegaConf\n",
    "from diff_exp.transforms_utils import get_transform\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "from diff_exp.utils import TransformDataset, tensor2pil\n",
    "from torchvision import transforms as tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eee37a9-db7d-4671-9e98-eb4221d675da",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")  # You need to download this file\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f2a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize_lips(img, predictor, lips_colors=(0, 255, 0), alpha=0.5):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    img_copy = img.copy()\n",
    "    \n",
    "    faces = detector(gray)\n",
    "    face = faces[0]\n",
    "    shape = predictor(gray, face)\n",
    "    \n",
    "    lips_landmarks = shape.parts()[48:68]\n",
    "    lips_points = [(x.x, x.y) for x in lips_landmarks]\n",
    "    lips_points = np.array(lips_points, dtype=np.int32)\n",
    "\n",
    "    cv2.fillPoly(img_copy, [lips_points], color=lips_colors)\n",
    "    cv2.addWeighted(img_copy, alpha, img, 1 - alpha, 0, img_copy)\n",
    "    return img_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19c15bbb-b758-4680-964b-dfa7b2de1db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_attr: Smiling\n",
      "data_dir: ../data\n",
      "split: train\n",
      "filter_path: null\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = default_args()\n",
    "args = OmegaConf.create(args)\n",
    "\n",
    "args.data_dir = \"../data\"\n",
    "\n",
    "print(OmegaConf.to_yaml(args))\n",
    "dataset = Dataset(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28e01c8b-1b96-460e-ad56-03d1df4c8e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_str = \"\"\"\n",
    "- - center_crop\n",
    "  - size: 178\n",
    "- - resize\n",
    "  - size: 64\n",
    "\"\"\"\n",
    "transform_cfg = yaml.load(transform_str, yaml.Loader)\n",
    "transform_cfg = OmegaConf.create(transform_cfg)\n",
    "transform = get_transform(transform_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131369a5-a7d3-453d-a64b-8510735ef1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Num failed: 1686/59621:  37%|████████████████████████████▏                                                | 59621/162770 [03:49<06:37, 259.81it/s]"
     ]
    }
   ],
   "source": [
    "images_out = []\n",
    "labels_out = []\n",
    "\n",
    "pbar = tqdm(total=len(dataset))\n",
    "n_failed = 0\n",
    "tot = 0\n",
    "\n",
    "for x, y in dataset:\n",
    "    x_np = np.array(x)\n",
    "    pbar.update()\n",
    "    tot += 1\n",
    "    try:\n",
    "        out = colorize_lips(x_np, predictor, lips_colors=(0, 255, 0), alpha=0.5)\n",
    "\n",
    "    except Exception as e:\n",
    "        n_failed += 1\n",
    "        continue\n",
    "\n",
    "    pbar.set_description(f\"Num failed: {n_failed}/{tot}\")\n",
    "    \n",
    "\n",
    "    out = Image.fromarray(out)\n",
    "    out = transform(out)\n",
    "    x = transform(x)\n",
    "\n",
    "    \n",
    "\n",
    "    images_out.append(x)\n",
    "    images_out.append(out)\n",
    "\n",
    "    labels_out.append(0)\n",
    "    labels_out.append(1)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b94c2bc-95c7-4a5c-a891-a6c5bf1638dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4492"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset) - len(images_out) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44dfb4cd-3e64-46aa-a925-f316f44effa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_images = [np.array(x) for x in images_out]\n",
    "all_images = np.stack(np_images, axis=0)\n",
    "all_labels = np.array(labels_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8aa6bdb3-8894-4c15-a63c-4bf50d1ffe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"green_lips_train.npz\", all_images, all_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
